{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1e45c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"JUPITER_MASTER_SPECTRA.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74908cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['target', 'wavelength', 'flux', 'notes'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b4dca33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target <class 'str'>\n",
      "wavelength <class 'numpy.ndarray'>\n",
      "flux <class 'numpy.ndarray'>\n",
      "notes <class 'str'>\n"
     ]
    }
   ],
   "source": [
    "for k, v in data.items():\n",
    "    print(k, type(v))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13e0670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: JUPITER | points: 1024 | device: cpu\n",
      "Synthetic dataset: (1275, 3, 1024) (225, 3, 1024)\n",
      "Label prevalence (train):\n",
      "CH4: 0.518\n",
      "NH3: 0.496\n",
      "C2H2: 0.536\n",
      "C2H6: 0.518\n",
      "MLP | epoch 01 | val_loss=0.2431 | microF1=0.909\n",
      "MLP | epoch 02 | val_loss=0.1464 | microF1=0.932\n",
      "MLP | epoch 03 | val_loss=0.1213 | microF1=0.946\n",
      "MLP | epoch 04 | val_loss=0.1198 | microF1=0.951\n",
      "MLP | epoch 05 | val_loss=0.1012 | microF1=0.958\n",
      "MLP | epoch 06 | val_loss=0.1119 | microF1=0.954\n",
      "MLP | epoch 07 | val_loss=0.1033 | microF1=0.953\n",
      "CNN1D | epoch 01 | val_loss=0.6889 | microF1=0.626\n",
      "CNN1D | epoch 02 | val_loss=0.6752 | microF1=0.591\n",
      "CNN1D | epoch 03 | val_loss=0.6451 | microF1=0.645\n",
      "CNN1D | epoch 04 | val_loss=0.6169 | microF1=0.658\n",
      "CNN1D | epoch 05 | val_loss=0.6079 | microF1=0.676\n",
      "CNN1D | epoch 06 | val_loss=0.5981 | microF1=0.640\n",
      "CNN1D | epoch 07 | val_loss=0.5903 | microF1=0.676\n",
      "GRU | epoch 01 | val_loss=0.6912 | microF1=0.470\n",
      "GRU | epoch 02 | val_loss=0.6860 | microF1=0.484\n",
      "GRU | epoch 03 | val_loss=0.6762 | microF1=0.591\n",
      "GRU | epoch 04 | val_loss=0.6459 | microF1=0.618\n",
      "GRU | epoch 05 | val_loss=0.6328 | microF1=0.594\n",
      "GRU | epoch 06 | val_loss=0.6207 | microF1=0.629\n",
      "GRU | epoch 07 | val_loss=0.6108 | microF1=0.642\n",
      "\n",
      "=== Model comparison (lower loss better) ===\n",
      "MLP   | val_loss=0.1012 | microF1=0.958 | train_time=0.6s\n",
      "CNN1D | val_loss=0.5903 | microF1=0.676 | train_time=8.6s\n",
      "GRU   | val_loss=0.6108 | microF1=0.642 | train_time=63.9s\n",
      "\n",
      "Best model: MLP | val_loss=0.1012 | microF1=0.958\n",
      "\n",
      "Predicted probabilities on REAL Jupiter spectrum:\n",
      " NH3: 0.999\n",
      "C2H6: 0.995\n",
      "C2H2: 0.903\n",
      " CH4: 0.001\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# 0) Imports + config\n",
    "# =========================\n",
    "import pickle, numpy as np, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "PKL_PATH = \"JUPITER_MASTER_SPECTRA.pkl\"\n",
    "\n",
    "# Speed knobs\n",
    "SEED = 7\n",
    "N_RESAMPLE = 1024        # 512 if you want even faster\n",
    "N_SYNTH = 1500           # 800–2000 is plenty for dry run\n",
    "EPOCHS = 7               # 5–10\n",
    "BATCH = 128\n",
    "LR = 1e-3\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "rng = np.random.default_rng(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "# Label space: Jupiter UV -> atmospheric species (not \"elements/minerals\")\n",
    "SPECIES = [\"CH4\", \"NH3\", \"C2H2\", \"C2H6\"]\n",
    "\n",
    "# Simple band templates (refine later using real line/band libraries)\n",
    "BANDS = {\n",
    "    \"CH4\": [(2350, 220), (2750, 260)],\n",
    "    \"NH3\": [(2050,  90), (2150,  90)],\n",
    "    \"C2H2\": [(2700, 110), (2810, 100)],\n",
    "    \"C2H6\": [(2400, 140), (2550, 120)],\n",
    "}\n",
    "\n",
    "def gaussian(x, mu, sigma):\n",
    "    return np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "\n",
    "# =========================\n",
    "# 1) Load + clean real spectrum\n",
    "# =========================\n",
    "with open(PKL_PATH, \"rb\") as f:\n",
    "    real = pickle.load(f)\n",
    "\n",
    "w = np.asarray(real[\"wavelength\"], dtype=float)\n",
    "f = np.asarray(real[\"flux\"], dtype=float)\n",
    "\n",
    "mask = np.isfinite(w) & np.isfinite(f)\n",
    "w, f = w[mask], f[mask]\n",
    "idx = np.argsort(w)\n",
    "w, f = w[idx], f[idx]\n",
    "\n",
    "print(\"Loaded:\", real.get(\"target\", \"unknown\"), \"| points:\", len(w), \"| device:\", device)\n",
    "\n",
    "# =========================\n",
    "# 2) Resample to fixed length (fast + consistent)\n",
    "# =========================\n",
    "def resample_to_fixed(wave, flux, n=N_RESAMPLE):\n",
    "    w_new = np.linspace(wave.min(), wave.max(), n)\n",
    "    f_new = np.interp(w_new, wave, flux)\n",
    "    return w_new.astype(np.float32), f_new.astype(np.float32)\n",
    "\n",
    "w_fix, f_fix = resample_to_fixed(w, f, N_RESAMPLE)\n",
    "\n",
    "# =========================\n",
    "# 3) Preprocess -> channels (C, N)\n",
    "# =========================\n",
    "def robust_norm(x):\n",
    "    med = np.median(x)\n",
    "    iqr = np.percentile(x, 75) - np.percentile(x, 25)\n",
    "    if iqr <= 0:\n",
    "        iqr = 1.0\n",
    "    return (x - med) / iqr\n",
    "\n",
    "def make_channels(wave, flux):\n",
    "    x = robust_norm(flux)\n",
    "    d1 = np.gradient(x, wave)\n",
    "    d2 = np.gradient(d1, wave)\n",
    "    X = np.stack([x, d1, d2], axis=0).astype(np.float32)  # (3, N)\n",
    "    return X\n",
    "\n",
    "X_real = make_channels(w_fix, f_fix)   # (3, N)\n",
    "X_real_t = torch.tensor(X_real[None, ...], dtype=torch.float32).to(device)  # (1,3,N)\n",
    "\n",
    "# =========================\n",
    "# 4) Synthetic data generator\n",
    "# =========================\n",
    "def sample_labels():\n",
    "    y = {sp: 0 for sp in SPECIES}\n",
    "    # independent balanced presence\n",
    "    for sp in SPECIES:\n",
    "        y[sp] = 1 if rng.random() < 0.5 else 0\n",
    "    if sum(y.values()) == 0:\n",
    "        y[rng.choice(SPECIES)] = 1\n",
    "    return y\n",
    "\n",
    "def synth_spectrum(wave, labels):\n",
    "    # continuum: gentle curve + offset\n",
    "    x = (wave - wave.min()) / (wave.max() - wave.min())\n",
    "    cont = 1.0 + 0.08 * (x - 0.5) + 0.05 * (x - 0.5) ** 2\n",
    "    cont += rng.normal(0, 0.01)\n",
    "\n",
    "    spec = cont.copy()\n",
    "\n",
    "    # multiplicative absorption dips\n",
    "    for sp, present in labels.items():\n",
    "        if not present:\n",
    "            continue\n",
    "        for (c, w0) in BANDS[sp]:\n",
    "            # --- CH4 stronger + broader (key change) ---\n",
    "            if sp == \"CH4\":\n",
    "                depth = rng.uniform(0.08, 0.25)\n",
    "                width = w0 * rng.uniform(0.9, 1.6)\n",
    "            else:\n",
    "                depth = rng.uniform(0.03, 0.15)\n",
    "                width = w0 * rng.uniform(0.7, 1.3)\n",
    "\n",
    "            dip = 1.0 - depth * gaussian(wave, c, width)\n",
    "            spec *= dip\n",
    "\n",
    "    # point noise + mild correlation\n",
    "    noise = rng.normal(0, 0.01, size=wave.shape[0])\n",
    "    noise = np.convolve(noise, np.ones(7)/7, mode=\"same\")\n",
    "    spec = spec + noise\n",
    "    return spec.astype(np.float32)\n",
    "\n",
    "def build_dataset(wave, n=N_SYNTH):\n",
    "    X_list, Y_list = [], []\n",
    "    for _ in range(n):\n",
    "        lab = sample_labels()\n",
    "        spec = synth_spectrum(wave, lab)\n",
    "        X = make_channels(wave, spec)  # (3, N)\n",
    "        y = np.array([lab[sp] for sp in SPECIES], dtype=np.float32)\n",
    "        X_list.append(X)\n",
    "        Y_list.append(y)\n",
    "    X = np.stack(X_list, axis=0)  # (B, 3, N)\n",
    "    Y = np.stack(Y_list, axis=0)  # (B, K)\n",
    "    return X, Y\n",
    "\n",
    "X, Y = build_dataset(w_fix, N_SYNTH)\n",
    "\n",
    "perm = rng.permutation(len(X))\n",
    "X, Y = X[perm], Y[perm]\n",
    "n_train = int(0.85 * len(X))\n",
    "X_train, Y_train = X[:n_train], Y[:n_train]\n",
    "X_val,   Y_val   = X[n_train:], Y[n_train:]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_train), torch.tensor(Y_train)),\n",
    "    batch_size=BATCH, shuffle=True\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    TensorDataset(torch.tensor(X_val), torch.tensor(Y_val)),\n",
    "    batch_size=BATCH, shuffle=False\n",
    ")\n",
    "\n",
    "print(\"Synthetic dataset:\", X_train.shape, X_val.shape)\n",
    "\n",
    "# Optional sanity check (highly recommended)\n",
    "print(\"Label prevalence (train):\")\n",
    "prev = Y_train.mean(axis=0)\n",
    "for sp, p in zip(SPECIES, prev):\n",
    "    print(f\"{sp}: {float(p):.3f}\")\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 5) Models: MLP, CNN, GRU\n",
    "# =========================\n",
    "K = len(SPECIES)\n",
    "C = 3\n",
    "N = N_RESAMPLE\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, c=C, n=N, k=K):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(c*n, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.15),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, k)\n",
    "        )\n",
    "    def forward(self, x):  # x: (B,C,N)\n",
    "        return self.net(x)\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, in_ch=C, k=K):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, 32, kernel_size=7, padding=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.AdaptiveAvgPool1d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, k)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, c=C, k=K, hidden=64):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=c, hidden_size=hidden, num_layers=1,\n",
    "                          batch_first=True, bidirectional=False)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(hidden, k)\n",
    "        )\n",
    "    def forward(self, x):  # x: (B,C,N) -> (B,N,C)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        _, h = self.gru(x)          # h: (1,B,H)\n",
    "        h = h[-1]                   # (B,H)\n",
    "        return self.head(h)\n",
    "\n",
    "# =========================\n",
    "# 6) Train + evaluate utilities\n",
    "# =========================\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def micro_f1_from_logits(logits, y_true, thr=0.5):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    y_hat = (probs >= thr).float()\n",
    "    tp = (y_hat * y_true).sum()\n",
    "    fp = (y_hat * (1 - y_true)).sum()\n",
    "    fn = ((1 - y_hat) * y_true).sum()\n",
    "    denom = (2*tp + fp + fn).clamp(min=1e-8)\n",
    "    return (2*tp / denom).item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    all_logits, all_y = [], []\n",
    "    for xb, yb in val_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = model(xb)\n",
    "        total_loss += loss_fn(logits, yb).item() * xb.size(0)\n",
    "        all_logits.append(logits)\n",
    "        all_y.append(yb)\n",
    "    total_loss /= len(X_val)\n",
    "    logits = torch.cat(all_logits, dim=0)\n",
    "    y_true = torch.cat(all_y, dim=0)\n",
    "    f1 = micro_f1_from_logits(logits, y_true)\n",
    "    return total_loss, f1\n",
    "\n",
    "def train_model(model, name):\n",
    "    model.to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    best = {\"loss\": 1e9, \"state\": None, \"f1\": 0.0}\n",
    "    t0 = time.time()\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad()\n",
    "            logits = model(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "        vloss, vf1 = evaluate(model)\n",
    "        if vloss < best[\"loss\"]:\n",
    "            best = {\"loss\": vloss, \"state\": {k: v.detach().cpu() for k, v in model.state_dict().items()}, \"f1\": vf1}\n",
    "        print(f\"{name} | epoch {epoch:02d} | val_loss={vloss:.4f} | microF1={vf1:.3f}\")\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    return best, dt\n",
    "\n",
    "# =========================\n",
    "# 7) Train all three\n",
    "# =========================\n",
    "results = []\n",
    "\n",
    "mlp = MLP()\n",
    "best_mlp, t_mlp = train_model(mlp, \"MLP\")\n",
    "results.append((\"MLP\", best_mlp[\"loss\"], best_mlp[\"f1\"], t_mlp, best_mlp))\n",
    "\n",
    "cnn = CNN1D()\n",
    "best_cnn, t_cnn = train_model(cnn, \"CNN1D\")\n",
    "results.append((\"CNN1D\", best_cnn[\"loss\"], best_cnn[\"f1\"], t_cnn, best_cnn))\n",
    "\n",
    "gru = GRUClassifier(hidden=64)\n",
    "best_gru, t_gru = train_model(gru, \"GRU\")\n",
    "results.append((\"GRU\", best_gru[\"loss\"], best_gru[\"f1\"], t_gru, best_gru))\n",
    "\n",
    "print(\"\\n=== Model comparison (lower loss better) ===\")\n",
    "for name, lossv, f1v, dt, _ in sorted(results, key=lambda x: x[1]):\n",
    "    print(f\"{name:5s} | val_loss={lossv:.4f} | microF1={f1v:.3f} | train_time={dt:.1f}s\")\n",
    "\n",
    "# pick best by val_loss\n",
    "best_name, best_loss, best_f1, best_dt, best_blob = sorted(results, key=lambda x: x[1])[0]\n",
    "print(f\"\\nBest model: {best_name} | val_loss={best_loss:.4f} | microF1={best_f1:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 8) Inference on real Jupiter spectrum\n",
    "# =========================\n",
    "def load_best_model(name, state):\n",
    "    if name == \"MLP\":\n",
    "        m = MLP()\n",
    "    elif name == \"CNN1D\":\n",
    "        m = CNN1D()\n",
    "    else:\n",
    "        m = GRUClassifier(hidden=64)\n",
    "    m.load_state_dict(state)\n",
    "    m.to(device).eval()\n",
    "    return m\n",
    "\n",
    "best_model = load_best_model(best_name, best_blob[\"state\"])\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = best_model(X_real_t)[0]\n",
    "    probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "print(\"\\nPredicted probabilities on REAL Jupiter spectrum:\")\n",
    "for sp, p in sorted(zip(SPECIES, probs), key=lambda x: -x[1]):\n",
    "    print(f\"{sp:>4}: {p:.3f}\")\n",
    "\n",
    "# =========================\n",
    "# 9) Save best model artifact (optional)\n",
    "# =========================\n",
    "# save_path = f\"/mnt/data/{real.get('target','TARGET').upper()}_{best_name}_best.pt\"\n",
    "# torch.save({\n",
    "#     \"model_type\": best_name,\n",
    "#     \"state_dict\": best_blob[\"state\"],\n",
    "#     \"species\": SPECIES,\n",
    "#     \"n_resample\": N_RESAMPLE,\n",
    "#     \"notes\": \"Trained on synthetic band-mixtures; dry-run model.\"\n",
    "# }, save_path)\n",
    "# print(\"\\nSaved best model to:\", save_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
